{% if target2string %}
target_test = target_test.astype(str)
{% endif %}
{% if pipeline.adaptation_metric == macros.Metric.AUC.value %}
from sklearn.metrics import roc_auc_score
{% if pipeline.task.is_multiclass == True %}
auc = roc_auc_score(target_test.values.ravel(), y_pred, multi_class="ovr")
{% else %}
auc = roc_auc_score(target_test, y_pred)
{% endif %}
print('RESULT: AUC Score: ' + str(auc))
{% elif (pipeline.adaptation_metric == macros.Metric.Accuracy.value) and (not pipeline.is_multi_class_multi_targets) %}
from sklearn.metrics import accuracy_score

accuracy = accuracy_score(target_test, y_pred)
print('RESULT: Accuracy: ' + str(accuracy))
{% elif (pipeline.adaptation_metric == macros.Metric.Accuracy.value) and (pipeline.is_multi_class_multi_targets) %}
from sklearn.metrics import accuracy_score

__accs = []
for i, col in enumerate(target_test.columns):
    one_acc = accuracy_score(target_test[col], y_pred[:, i:i+1])
    __accs.append(one_acc)
print(f"RESULT: Accuracy : {str(sum(__accs)/len(__accs))}")
{% elif pipeline.adaptation_metric == macros.Metric.F1.value %}
from sklearn import metrics

f1 = metrics.f1_score(target_test, y_pred, average='macro')
print('RESULT: F1 Score: ' + str(f1))
{% elif pipeline.adaptation_metric == macros.Metric.R2.value %}
from sklearn import metrics

r2 = metrics.r2_score(target_test, y_pred)
print('RESULT: R2 Score:', str(r2))
{% elif pipeline.adaptation_metric == macros.Metric.RMSE.value %}
from sklearn.metrics import mean_squared_error

rmse = mean_squared_error(target_test, y_pred, squared=False)
print('RESULT: RMSE:', str(rmse))
{% elif pipeline.adaptation_metric == macros.Metric.RMSLE.value %}
import numpy as np
from sklearn.metrics import mean_squared_log_error

target_test = np.clip(target_test, 0, None)
y_pred = np.clip(y_pred, 0, None)
rmsle = np.sqrt(mean_squared_log_error(target_test, y_pred))
print('RESULT: RMSLE:', str(rmsle))
{% elif pipeline.adaptation_metric == macros.Metric.Gini.value %}
from sklearn.metrics import roc_auc_score
{% if pipeline.task.is_multiclass == True %}
gini = 2 * roc_auc_score(target_test.values.ravel(), y_pred, multi_class="ovr") - 1
{% else %}
gini = 2 * roc_auc_score(target_test, y_pred) - 1
{% endif %}
print('RESULT: Gini: ' + str(gini))
{% elif pipeline.adaptation_metric == macros.Metric.MAE.value %}
from sklearn.metrics import mean_absolute_error

mae = mean_absolute_error(target_test, y_pred)
print('RESULT: MAE:', str(mae))
{% elif pipeline.adaptation_metric == macros.Metric.LogLoss.value %}
from sklearn.metrics import log_loss

log_loss = log_loss(target_test, y_pred)
print('RESULT: Log Loss:', str(log_loss))
{% elif pipeline.adaptation_metric == macros.Metric.ROC_AUC.value %}
from sklearn.metrics import roc_auc_score
{% if pipeline.task.is_multiclass == True %}
__roc_auc = roc_auc_score(target_test.values.ravel(), y_pred, multi_class="ovr")
{% else %}
__roc_auc = roc_auc_score(target_test, y_pred)
{% endif %}
print('RESULT: ROC AUC:', str(__roc_auc))
{% elif pipeline.adaptation_metric == macros.Metric.MCC.value %}
from sklearn.metrics import matthews_corrcoef

mcc = matthews_corrcoef(target_test, y_pred)
print('RESULT: MCC:', str(mcc))
{% elif pipeline.adaptation_metric.startswith("MAP_") %}
{% set k = pipeline.adaptation_metric.split("_")[1] %}
def apk(actual, predicted, k):
    if len(predicted)>k:
        predicted = predicted[:k]

    score = 0.0
    num_hits = 0.0

    for i,p in enumerate(predicted):
        if p in actual and p not in predicted[:i]:
            num_hits += 1.0
            score += num_hits / (i+1.0)

    return score / min(len(actual), k)

def mapk(actual, predicted, k):
    """ Computes the mean average precision at k.

    Args:
        actual (list[list[str] or ndarray): A list of lists of elements that are to be predicted
        predicted (list[list[str] or ndarray): A list of lists of predicted elements
            (In each list, arrange in the order you predicted.)
        k (int): The maximum number of predicted elements

    Returns:
        double: The mean average precision at k over the input lists
    """
    return np.mean([apk(a,p,k) for a,p in zip(actual, predicted)])

map_k = mapk(target_test.to_numpy(), y_pred, k={{ k }})
print('RESULT: MAP@K: ' + str(map_k))
{% elif pipeline.adaptation_metric == macros.Metric.QWK.value %}
from sklearn.metrics import cohen_kappa_score

qwk = cohen_kappa_score(target_test, y_pred, weights='quadratic')
print('RESULT: QWK:', str(qwk))
{% elif pipeline.task_type == macros.TASK_REGRESSION.value %}
from sklearn import metrics

r2 = metrics.r2_score(target_test, y_pred)
print('RESULT: R2 Score:', str(r2))
{% else %}
from sklearn import metrics

f1 = metrics.f1_score(target_test, y_pred, average='macro')
print('RESULT: F1 Score: ' + str(f1))
{% endif %}